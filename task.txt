Customer Service App with Utility Nuances

Main features:
1: An AI Voice Agent
    - The AI Voice Agent acts as expected, providing customer service for customers, drawing info from a backend database to help customers with their specifics.


2: Copilot Assistant Agent.
    - The Copilot Assistant Agent acts in assistance to a human representative, presenting info to the human rep in a sidebar based on the real-time conversation. It only acts to nudge the human rep in the correct direction.
    
    Implementation: If the customer service problem requires the customer ID, the AI Agent will pull it from the database, and the Copilot will pull up background information on the customer for the human rep.

3: Voice Call vs. Text Chat
    - Users will get to choose whether they want voice calling service or chatbot service.

4: Active Service Options
    - The customer has the option in real time to switch between one agent or the other, and the app should track the number of switches as part of a service diagnostic.

5: Shared Single System
    - There is a single backend, and the option between two agents.



This is a sophisticated "Human-in-the-Loop" (HITL) architecture. The critical engineering challenge here is not just building two agents, but building the bridge that allows them to share context and audio streams instantly without dropping the call or losing chat history.

Here is the architectural blueprint and technology selection to build this system.

High-Level Architecture: The "Conference" Pattern
To achieve Feature 4 (Active Service Options) where you can switch in real-time, you cannot simply "forward" a call. Forwarding disconnects one leg to start another.

Instead, you must use a Conference Bridge architecture.

The Customer calls in and is placed into a digital conference room.

The AI Agent joins the conference immediately (as a silent listener + speaker).

The Human Rep joins the same conference (initially muted).

The Switch: When the customer toggles the option, your backend simply mutes the AI's audio stream and unmutes the Human Rep's audio stream (or vice versa). The customer never hears a click, hold music, or transfer tone.

1. Technology Stack & APIs
A. The AI Voice Agent (Feature 1)
For a truly conversational experience with <500ms latency, the "transcribe-then-think-then-speak" pipeline is too slow. You need a streaming WebSocket architecture.

Primary Choice: OpenAI Realtime API (WebRTC/WebSocket)

Why: It handles Speech-to-Text (STT), Logic (LLM), and Text-to-Speech (TTS) in a single WebSocket connection. This drastically reduces latency compared to stitching three separate services together.

Integration: You will use Twilio Media Streams. Twilio forks the raw audio from the phone call and streams it via WebSocket to your backend, which then relays it to OpenAI.

Alternative (Granular Control): Deepgram + LLM + ElevenLabs

Why: If OpenAI's voice costs are too high or you need specific custom voices.

STT: Deepgram Nova-2 (Fastest real-time transcription).

LLM: Groq (running Llama 3) or GPT-4o.

TTS: ElevenLabs Turbo v2.5 (Lowest latency high-quality voice).

B. The Copilot & Sidebar (Feature 2)
The Copilot needs to "hear" the conversation even when the human is talking.

Transcription: Use the same Deepgram or OpenAI stream. Even when the AI Agent is "off" (muted), the audio stream continues to be transcribed so the Copilot can generate suggestions.

RAG Engine: Pinecone or Weaviate (Vector Database).

Workflow: When the customer mentions "Order #123," the Copilot vector-searches your database for that ID, retrieves the shipping status, and pushes a JSON card to the Human Rep's frontend via WebSocket.

C. Telephony & Messaging (Features 3 & 5)
Telephony Provider: Twilio Programmable Voice.

Specifically, use the Twilio Conference API to manage the multi-party state described above.

Chat Provider: Twilio Conversations API or Stream Chat.

These APIs unify SMS, WhatsApp, and Web Chat into a single thread, making it easy to feed text history into the Copilot.

2. Implementation Strategy by Feature
Feature 1 & 2: Shared Backend & Data Flow
You need a single backend (Node.js or Python/FastAPI are best for this) that manages the "State" of the conversation.

Unified Context Object: Create a JSON object in a fast in-memory store (like Redis) that holds the current state.

JSON
{
  "call_sid": "CA12345...",
  "customer_id": "CUST_99",
  "current_mode": "AI_AGENT", // or "HUMAN_REP"
  "transcript_summary": "User is asking about a refund...",
  "suggested_actions": ["Process Refund", "Check Policy"]
}
Copilot Logic: Your backend watches the live transcript. If a specific keyword (intent) is detected (e.g., "cancel subscription"), the Copilot triggers a background search and updates the suggested_actions in Redis. The frontend listens to Redis updates via Socket.io and renders the suggestion in the sidebar.

Feature 3: Voice vs. Text Chat
Text Chat: This is simpler. The AI Agent is just a loop that reads incoming text and posts a response.

The Nuance: The Copilot must work for both. By normalizing both Voice (transcribed to text) and Chat (already text) into a standard "Message Object," your Copilot logic remains the same regardless of the input channel.

Feature 4: The Real-Time Switch (The "Diagnostic")
This is the core differentiator. Here is the logic flow for the "Switch":

Trigger: User presses "Talk to Human" in the app (or says "I want a person").

Backend Action:

Sends API call to Twilio: updateParticipant(AI_Agent_Call_SID, { muted: true }).

Sends API call to Twilio: updateParticipant(Human_Rep_Call_SID, { muted: false }).

Database Write: INSERT INTO switch_logs (call_id, timestamp, direction, reason) VALUES (...).

Frontend Update: A WebSocket event fires to the Human Rep's screen, changing the UI from "Monitoring Mode" (Gray) to "Active Call Mode" (Green).

Diagnostic: You can now query the switch_logs table to see exactly at what timestamp and conversation turn the user felt the need to switch.

3. Recommended Tech Stack Summary
Component	Recommended Technology	Why?
Backend Core	Node.js (TypeScript)	Best ecosystem for WebSockets (Socket.io) and handling async Twilio events simultaneously.
Telephony	Twilio Programmable Voice	Industry standard for creating conference bridges and media streams.
AI Speech API	OpenAI Realtime API	Lowest latency "Voice-to-Voice" experience currently available.
Transcription (Copilot)	Deepgram Nova-2	You need a secondary, cheaper transcription stream just for the Copilot text generation.
Vector DB (Knowledge)	Pinecone	Fast retrieval for the Copilot to pull customer policy/data.
Real-time Frontend	React + Socket.io	React handles the sidebar UI state updates efficiently; Socket.io handles the instant "nudges."
Database (Main)	PostgreSQL	Robust relational data for Customer IDs, logs, and switch diagnostics.
Next Step

Google Conversational Agents — Complete Setup with Twilio This video demonstrates the specific engineering pattern of connecting Twilio to an AI agent and handling the "handoff" logic, which is the most complex part of your request.

Human Handoff on Google Conversational Agents — Complete Setup with Twilio - YouTube
Raj Kapadia · 91 views


Design the architecture of the project, listing directories and files.

This architecture follows a Monorepo structure. This is critical for your "Shared Single System" requirement, allowing your Backend (Node.js) and Frontend (React) to share TypeScript interfaces, database schemas, and WebSocket event types.

We will use TypeScript across the stack to ensure type safety between the AI's "Context Object" and the Frontend's "Sidebar."

Root Directory Structure
Plaintext
/customer-service-platform
├── /apps
│   ├── /backend             # Node.js/Express server (The Brain)
│   └── /web-client          # React App (Agent Dashboard & Customer Widget)
├── /packages
│   ├── /database            # Prisma ORM & Database migrations (Postgres)
│   └── /shared-types        # Shared Interfaces (DTOs, WebSocket Events)
├── package.json             # Monorepo workspaces config
└── docker-compose.yml       # Local dev (Redis, Postgres, Localstack)
1. The Backend (/apps/backend)
This is the most complex component. It manages the Twilio streams, the OpenAI WebSocket connection, and the state of the "Conference Bridge."

Plaintext
/apps/backend
├── src
│   ├── /config
│   │   └── env.ts               # API Keys (OpenAI, Twilio, Pinecone)
│   │
│   ├── /controllers             # REST API Endpoints
│   │   ├── callController.ts    # Incoming webhook for Twilio Calls
│   │   ├── chatController.ts    # Incoming text messages
│   │   └── switchController.ts  # Handles the "AI <-> Human" toggle logic
│   │
│   ├── /services
│   │   ├── /voice
│   │   │   ├── twilioClient.ts      # Twilio SDK wrapper
│   │   │   ├── conferenceManager.ts # **CRITICAL**: Manages Mute/Unmute logic for the switch
│   │   │   └── streamHandler.ts     # Handles Twilio <-> OpenAI WebSocket media streams
│   │   │
│   │   ├── /ai
│   │   │   ├── openAiRealtime.ts    # Manages the Voice Agent (WebSocket connection to OpenAI)
│   │   │   ├── promptBuilder.ts     # System instructions for the Voice Agent
│   │   │   └── transcriber.ts       # Deepgram service (for Copilot visibility when AI is muted)
│   │   │
│   │   ├── /copilot
│   │   │   ├── suggestionEngine.ts  # Logic to determine when to nudge the human
│   │   │   └── vectorStore.ts       # Pinecone integration (RAG) to fetch customer info
│   │   │
│   │   └── /state
│   │       └── sessionStore.ts      # Redis wrapper to hold active call context
│   │
│   ├── /sockets
│   │   └── agentGateway.ts      # Socket.io events pushing data to Human Rep's Frontend
│   │
│   ├── app.ts                   # Express App setup
│   └── server.ts                # Entry point
│
└── package.json
Key Files Explained:
conferenceManager.ts: This implements Feature 4. It contains functions like muteAiAgent() and unmuteHumanRep(). It interacts with the Twilio Conference API to modify participant properties in real-time without dropping the call.

streamHandler.ts: This receives raw audio bytes from the phone call (via Twilio Media Streams) and pipes them into openAiRealtime.ts.

suggestionEngine.ts: This runs in the background. It watches the live transcript (text) and queries vectorStore.ts to see if relevant info (like "Policy ID") should be pushed to the frontend sidebar.

2. The Frontend (/apps/web-client)
This React application serves two masters: the Customer (simple chat/call widget) and the Human Representative (complex dashboard).

Plaintext
/apps/web-client
├── src
│   ├── /components
│   │   ├── /customer-widget
│   │   │   ├── CallButton.tsx        # Triggers voice call
│   │   │   └── ChatWindow.tsx        # Standard text chat UI
│   │   │
│   │   ├── /agent-dashboard
│   │   │   ├── ActiveCallBanner.tsx  # Shows "AI Talking" or "You Talking" status
│   │   │   ├── LiveTranscript.tsx    # Scrolling text of the conversation
│   │   │   ├── SidebarCopilot.tsx    # **CRITICAL**: The "Nudge" UI
│   │   │   └── ControlPanel.tsx      # The "Take Over" toggle switch
│   │   │
│   │   └── /shared
│   │       └── ConnectionStatus.tsx
│   │
│   ├── /hooks
│   │   ├── useSocket.ts              # Custom hook for Socket.io connection
│   │   └── useCallState.ts           # Manages local state of the active call
│   │
│   ├── /pages
│   │   ├── AgentPortal.tsx
│   │   └── CustomerDemo.tsx
│   │
│   ├── App.tsx
│   └── main.tsx
│
└── package.json
Key Files Explained:
SidebarCopilot.tsx: This component subscribes to the copilot_suggestion WebSocket event. When the backend sends a JSON payload (e.g., customer details), this component renders a card in the sidebar.

ControlPanel.tsx: Contains the physical button that hits the switchController endpoint on the backend. It tracks the click count for the Diagnostics requirement.

3. The Database & Shared Types (/packages)
/packages/database/prisma/schema.prisma This file defines your Single Shared System data structure.

Code snippet
// Feature 5: Shared Backend Data Models

model Customer {
  id        String   @id @default(uuid())
  name      String
  calls     Call[]
  // ... other details
}

model Call {
  id          String   @id // The Twilio Call SID
  mode        CallMode @default(AI_AGENT) // Enum: AI_AGENT | HUMAN_REP
  transcript  String?  @db.Text
  switchLogs  SwitchLog[]
  createdAt   DateTime @default(now())
}

// Feature 4: Service Diagnostic Tracking
model SwitchLog {
  id          String   @id @default(uuid())
  callId      String
  call        Call     @relation(fields: [callId], references: [id])
  switchedAt  DateTime @default(now())
  direction   String   // "AI_TO_HUMAN" or "HUMAN_TO_AI"
  reason      String?  // Optional context (e.g., "Sentiment Negative")
}
/packages/shared-types/index.ts Ensures the backend and frontend agree on the data shape.

TypeScript
export interface CopilotSuggestion {
  type: 'INFO' | 'ACTION';
  title: string;
  content: string; // e.g., "Customer ID: 8821"
  confidenceScore: number;
}

export interface CallStateUpdate {
  callId: string;
  activeSpeaker: 'AI' | 'HUMAN' | 'CUSTOMER';
  isMuted: boolean;
}
Architectural Next Step
This structure separates concerns while keeping the "Switching" logic centralized in the backend.



Are there better alternatives to each of the APIs that we are planning to use?

Yes, there are powerful alternatives that often outperform the "Default Stack" (Twilio + OpenAI + Pinecone) in specific areas like latency, cost, or emotional expressiveness.

Here is a breakdown of the best alternatives for each layer of your architecture.

1. AI Voice Agent Alternatives
Current Plan: OpenAI Realtime API (WebRTC) The Problem: It is expensive (~$0.60/min) and you are locked into one model (GPT-4o).

Alternative	Best For...	Why it might be better
Retell AI / Vapi	Speed & "Glue"	These are "Voice Layer" platforms that wrap LLMs. They handle the VAD (Voice Activity Detection) and Telephony interruption logic better than a raw OpenAI connection. They offer ~800ms latency and allow you to swap models (Claude, GPT-4o, Llama 3) instantly.
Hume AI (EVI)	Emotion/Empathy	If your customer service requires empathy (e.g., healthcare, complaints), Hume's EVI model detects tone of voice (sarcasm, distress) and adjusts its response prosody. OpenAI is currently flatter in tone.
Deepgram + Groq	Lowest Latency	This is the "Modular" approach. Use Deepgram Nova-2 for STT + Groq (Llama 3) for Logic + ElevenLabs Turbo for TTS. This stack can achieve <500ms latency and is often cheaper than OpenAI's all-in-one API.
2. Telephony Alternatives (Voice Connectivity)
Current Plan: Twilio Programmable Voice The Problem: Twilio is the industry standard but is expensive ($0.014/min) and routes over the public internet, adding jitter.

Alternative	Best For...	Why it might be better
Telnyx	Cost & Network	Direct Replacement. Telnyx has a private global IP network (less jitter than Twilio) and is typically 30-50% cheaper. It supports "Media Streams" (WebSockets) exactly like Twilio, making migration easy ("TeXML" is compatible with Twilio's "TwiML").
SignalWire	Video/Low-Level	Built by the creators of FreeSWITCH. It offers extreme low-latency control and is very effective if you later decide to add Video to your service options.
3. Copilot Transcription Alternatives
Current Plan: Deepgram The Problem: Deepgram is excellent, but you might want consolidation or specific entity features.

Alternative	Best For...	Why it might be better
AssemblyAI	NLU & Summary	While slightly slower than Deepgram, AssemblyAI has superior "LeMUR" features for extracting structured data (like "Order IDs" or "Dates") from messy transcripts, which is critical for your Copilot's accuracy.
Gladia	Multilingual	If you have a global customer base, Gladia's real-time transcription often outperforms competitors in switching languages mid-sentence (code-switching).
4. Vector Database Alternatives (Knowledge)
Current Plan: Pinecone The Problem: Pinecone is expensive at scale and adds another external service to manage.

Alternative	Best For...	Why it might be better
pgvector (Postgres)	Simplicity	Since you are likely using PostgreSQL for your main database, enabling the pgvector extension keeps your data and vectors in one place. It eliminates data sync issues and is "free" if you already pay for the DB instance.
Weaviate	Hybrid Search	If customers search for "policy about refunds" (semantic) but also "Order #9921" (exact match), Weaviate's hybrid search (Keyword + Vector) is often more accurate than Pinecone's pure vector search.
Recommendation: The "High-Performance" Stack
If you want to build the absolute best Customer Service experience, I recommend this specific combination to replace the defaults:

Telephony: Telnyx (Better audio network, lower cost, supports Media Streams).

Voice Brain: Retell AI (Or Vapi).

Why: Instead of building the "Conference Bridge" logic from scratch, Retell handles the "AI Agent" leg entirely. You just "transfer" the call to Retell.

Note: Retell exposes the live transcript via webhooks, which you can pump into your Copilot.

Copilot Brain: AssemblyAI (For extracting intents from the transcript) + pgvector (For looking up customer data).

Updated Architecture Diagram (with Alternatives)
Customer Call → Telnyx (Phone Network)

Stream 1 (Audio) → Retell AI (Handles Talking/Listening)

Stream 2 (Forked Audio) → Your Backend (Node.js)

Backend → AssemblyAI (Transcribes for Human Rep)

Backend → pgvector (Searches Order Info)

Backend → Frontend (Socket.io updates Sidebar)

This video benchmarks the latency of the specific "Voice API" layers (Retell, Vapi) which is the biggest bottleneck in your system. Retell AI vs. VAPI: My Top 4 Reasons to Switch! This analysis is relevant because it compares the exact "AI Voice Layer" alternatives (Retell vs. Vapi) that would replace your manual "OpenAI + Twilio" setup, highlighting latency and interruption handling which are critical for customer service.

Webinar - Real-Time Media Streaming to Power AI - YouTube
Telnyx · 300 views


This is a **solid recommendation** that I agree with. Let me break down why this "High-Performance" stack is actually a **Simplicity** stack in disguise:

---

## What Changes

| Component | Original | High-Performance | Complexity Saved |
|-----------|----------|------------------|------------------|
| **Telephony** | Twilio | Telnyx | Minor (similar APIs, better pricing) |
| **Voice AI** | OpenAI Realtime + Custom Conference Bridge | Retell AI / Vapi | **MAJOR** - eliminates 3-4 custom services |
| **Transcription** | Deepgram | AssemblyAI | Built-in intent extraction |
| **Vector DB** | Pinecone (separate service) | pgvector (in PostgreSQL) | One less service to manage |

---

## Why This Is Better for Most Teams

### 1. Retell/Vapi Abstracts the Hardest Part

The original architecture required you to build:
- `conferenceManager.ts` (mute/unmute logic)
- `streamHandler.ts` (Twilio ↔ OpenAI audio piping)
- `openAiRealtime.ts` (WebSocket management)

**Retell handles all of this.** You just:
```
Call comes in → Transfer to Retell → Retell talks to customer
                                   ↘ Webhooks send transcript to your backend
```

### 2. pgvector Keeps Everything in One Database

```sql
-- No separate Pinecone service needed
SELECT * FROM customers 
WHERE embedding <-> query_embedding < 0.5
ORDER BY embedding <-> query_embedding
LIMIT 5;
```

One PostgreSQL instance = relational data + vector search + switch logs.

### 3. AssemblyAI's LeMUR = Built-in Intent Extraction

Instead of custom `suggestionEngine.ts` logic, AssemblyAI can directly output:
```json
{
  "intent": "refund_request",
  "entities": { "order_id": "12345" },
  "sentiment": "frustrated"
}
```

---

## Updated Architecture (Visual)

```
┌─────────────────────────────────────────────────────────────────┐
│                        CUSTOMER CALL                            │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
                    ┌─────────────────┐
                    │     TELNYX      │
                    │  (Phone Network) │
                    └────────┬────────┘
                             │
              ┌──────────────┴──────────────┐
              │                             │
              ▼                             ▼
    ┌─────────────────┐           ┌─────────────────┐
    │   RETELL AI     │           │   YOUR BACKEND  │
    │ (Voice Agent)   │           │    (Node.js)    │
    │                 │           │                 │
    │ • STT           │  webhook  │ • AssemblyAI    │
    │ • LLM           │ ────────► │ • pgvector      │
    │ • TTS           │ transcript│ • Session State │
    └─────────────────┘           └────────┬────────┘
                                           │
                                           │ Socket.io
                                           ▼
                                  ┌─────────────────┐
                                  │    FRONTEND     │
                                  │ (React Sidebar) │
                                  └─────────────────┘
```

---

## One Caveat to Verify

**The "Real-Time Switch" (Feature 4)**: Confirm that Retell/Vapi supports the seamless handoff pattern. Specifically:

- Can you **keep the customer on the line** while transferring control to a human?
- Does Retell expose a "conference" or "warm transfer" API?

Retell does advertise "human handoff," but verify it's the **same call leg** (no audible transfer) rather than a cold transfer.

---

## Verdict

✅ **Agreeable.** This stack is:
- Faster to build (weeks vs. months)
- Lower operational complexity (fewer services)
- Likely cheaper at scale (pgvector vs. Pinecone, Telnyx vs. Twilio)

The trade-off is **less fine-grained control** over the voice AI behavior—but for 90% of customer service use cases, Retell/Vapi's defaults are excellent.

Want me to re-map the file structure to reflect this simplified architecture?



I'll design a phased approach for building Version 0. This plan prioritizes **vertical slices** - getting a working end-to-end flow early, then expanding features.

---

## Build Plan: Version 0

### Phase 0: Foundation (Day 1)
**Goal**: Monorepo structure, local dev environment, type safety

| Step | Task | Output |
|------|------|--------|
| 0.1 | Initialize monorepo with pnpm workspaces | `package.json`, `pnpm-workspace.yaml` |
| 0.2 | Create folder structure | `/apps/backend`, `/apps/web-client`, `/packages/*` |
| 0.3 | Docker Compose for PostgreSQL + Redis | `docker-compose.yml` |
| 0.4 | Environment config | `.env.example`, `apps/backend/src/config/env.ts` |
| 0.5 | Shared types package | `packages/shared-types/src/index.ts` |

---

### Phase 1: Database Layer (Day 2)
**Goal**: Persistent storage with vector search capability

| Step | Task | Output |
|------|------|--------|
| 1.1 | Initialize Prisma | `packages/database/prisma/schema.prisma` |
| 1.2 | Define models (Customer, Call, SwitchLog) | Schema file |
| 1.3 | Enable pgvector extension | Migration + raw SQL |
| 1.4 | Seed script with test customers | `packages/database/seed.ts` |
| 1.5 | Generate Prisma client | Typed DB access |

---

### Phase 2: Backend Skeleton (Day 3)
**Goal**: Express server with health checks and session management

| Step | Task | Output |
|------|------|--------|
| 2.1 | Express + TypeScript setup | `apps/backend/src/app.ts` |
| 2.2 | Redis connection wrapper | `src/services/state/sessionStore.ts` |
| 2.3 | Health check endpoint | `GET /health` |
| 2.4 | Basic error handling middleware | `src/middleware/errorHandler.ts` |
| 2.5 | Socket.io server initialization | `src/sockets/agentGateway.ts` |

---

### Phase 3: Telephony - Telnyx (Day 4-5)
**Goal**: Receive incoming calls and respond with TwiML-equivalent

| Step | Task | Output |
|------|------|--------|
| 3.1 | Telnyx account + phone number | API credentials |
| 3.2 | Incoming call webhook | `src/controllers/callController.ts` |
| 3.3 | Answer call + play greeting | TeXML response |
| 3.4 | Create Call record in DB | Prisma insert |
| 3.5 | Ngrok/tunnel for local testing | Publicly accessible webhook |

**Milestone**: Call your number → hear "Welcome to support"

---

### Phase 4: Voice AI - Retell Integration (Day 6-8)
**Goal**: AI agent handles the conversation

| Step | Task | Output |
|------|------|--------|
| 4.1 | Retell account + agent setup | Agent ID, API key |
| 4.2 | Configure Retell agent persona/prompt | Retell dashboard |
| 4.3 | Transfer incoming call to Retell | `src/services/voice/retellClient.ts` |
| 4.4 | Webhook endpoint for Retell events | `src/controllers/retellWebhook.ts` |
| 4.5 | Store live transcript in Redis | Session update on each utterance |
| 4.6 | Handle call end event | Update Call record, clear session |

**Milestone**: Call → AI agent converses → transcript stored

---

### Phase 5: Copilot Brain (Day 9-11)
**Goal**: Extract intents, search customer data, generate suggestions

| Step | Task | Output |
|------|------|--------|
| 5.1 | AssemblyAI integration | `src/services/copilot/transcriptAnalyzer.ts` |
| 5.2 | Intent extraction from transcript chunks | LeMUR API call |
| 5.3 | pgvector embedding function | `src/services/copilot/vectorStore.ts` |
| 5.4 | Customer lookup by semantic query | SQL with `<->` operator |
| 5.5 | Suggestion engine logic | `src/services/copilot/suggestionEngine.ts` |
| 5.6 | Push suggestions via Socket.io | `copilot_suggestion` event |

**Milestone**: Customer says "order 12345" → Backend finds order → Pushes card to frontend

---

### Phase 6: Frontend - Agent Dashboard (Day 12-15)
**Goal**: Human rep sees live transcript and copilot suggestions

| Step | Task | Output |
|------|------|--------|
| 6.1 | Vite + React + TypeScript setup | `apps/web-client/` |
| 6.2 | Socket.io client hook | `src/hooks/useSocket.ts` |
| 6.3 | Agent portal page layout | `src/pages/AgentPortal.tsx` |
| 6.4 | Live transcript component | `src/components/LiveTranscript.tsx` |
| 6.5 | Sidebar copilot component | `src/components/SidebarCopilot.tsx` |
| 6.6 | Active call banner | `src/components/ActiveCallBanner.tsx` |
| 6.7 | Connection status indicator | Visual feedback |

**Milestone**: Open dashboard → See live transcript + suggestions appearing

---

### Phase 7: The Switch (Day 16-18)
**Goal**: Real-time toggle between AI and Human agent

| Step | Task | Output |
|------|------|--------|
| 7.1 | Research Retell handoff API | Confirm transfer pattern |
| 7.2 | Switch controller endpoint | `POST /api/switch` |
| 7.3 | Transfer call from Retell → Human (Telnyx conference) | `src/services/voice/conferenceManager.ts` |
| 7.4 | Log switch event to DB | SwitchLog insert |
| 7.5 | Control panel UI component | `src/components/ControlPanel.tsx` |
| 7.6 | UI state update on switch | Banner changes color |
| 7.7 | Reverse switch (Human → AI) | Re-transfer to Retell |

**Milestone**: Click "Take Over" → Human talks → Click "Hand Back" → AI resumes

---

### Phase 8: Text Chat Channel (Day 19-20)
**Goal**: Unified text chat alongside voice

| Step | Task | Output |
|------|------|--------|
| 8.1 | Chat message endpoint | `POST /api/chat` |
| 8.2 | Normalize to Message Object | Shared interface |
| 8.3 | Route to Retell (or direct LLM for text) | Conditional logic |
| 8.4 | Customer chat widget | `src/components/ChatWindow.tsx` |
| 8.5 | Customer demo page | `src/pages/CustomerDemo.tsx` |

**Milestone**: Customer types message → AI responds → Same copilot suggestions work

---

### Phase 9: Polish & Diagnostics (Day 21-23)
**Goal**: Production readiness

| Step | Task | Output |
|------|------|--------|
| 9.1 | Switch analytics query | Count switches per call |
| 9.2 | Simple analytics dashboard | Chart of switch frequency |
| 9.3 | Error handling + retries | Resilient API calls |
| 9.4 | Environment validation on startup | Fail fast if missing keys |
| 9.5 | README with setup instructions | Documentation |
| 9.6 | End-to-end test script | Manual QA checklist |

---

## Visual Timeline

```
Week 1          Week 2          Week 3          Week 4
─────────────────────────────────────────────────────────
[P0][P1][P2]    [P3  ][P4     ] [P5    ][P6         ]
 Foundation     Telephony+Voice  Copilot  Frontend
                                         
                                         [P7    ][P8 ][P9  ]
                                          Switch  Chat Polish
```

---

Great point. Testing the switch functionality is critical. Let me add a **Testing & Scripts** section to the plan:

---

## Addition: Phase 9.5 - Test Scripts & QA Infrastructure

### Test Trigger Mechanisms

| Channel | Trigger to Switch to Human | Trigger to Switch to AI |
|---------|---------------------------|------------------------|
| **Voice** | Say: *"I want to speak to a human"* or *"Transfer me"* | Say: *"Let me talk to the AI again"* or *"Go back to bot"* |
| **Voice (DTMF)** | Press `0` | Press `*` |
| **Text Chat** | Type: `/human` or *"speak to agent"* | Type: `/ai` or *"back to bot"* |

---

### Test Script Structure

Each script will be a markdown file with this format:

```
/test-scripts
├── voice/
│   ├── 01-happy-path-ai-resolves.md
│   ├── 02-escalation-to-human.md
│   ├── 03-multiple-switches.md
│   ├── 04-immediate-human-request.md
│   └── 05-complex-order-lookup.md
├── chat/
│   ├── 01-happy-path-ai-resolves.md
│   ├── 02-escalation-to-human.md
│   └── 03-multiple-switches.md
└── edge-cases/
    ├── switch-during-ai-speaking.md
    ├── rapid-switch-spam.md
    └── switch-with-no-human-available.md
```

---

### Sample Test Script Format

**File**: `test-scripts/voice/02-escalation-to-human.md`

```markdown
# Test Script: Escalation to Human

## Scenario
Customer calls about a refund, AI cannot resolve, customer requests human.

## Prerequisites
- [ ] Human rep dashboard open
- [ ] Test customer ID: `CUST_TEST_001`
- [ ] Order in DB: `ORD_12345` (status: delivered)

## Script

| Turn | Speaker | Action / Dialogue | Expected Result |
|------|---------|-------------------|-----------------|
| 1 | System | Call connects | AI greeting plays |
| 2 | Tester | "Hi, I need help with a refund" | AI asks for order number |
| 3 | Tester | "Order 12345" | AI confirms order, offers options |
| 4 | Tester | "I don't want those options, I want to talk to a real person" | **SWITCH TRIGGERED** |
| 5 | System | — | Banner turns GREEN on dashboard |
| 6 | System | — | Copilot shows customer info card |
| 7 | Human | "Hi, I see you're asking about order 12345..." | Conversation continues |
| 8 | Tester | "Actually, the bot can handle it. Go back to the AI" | **SWITCH BACK TRIGGERED** |
| 9 | System | — | Banner turns GRAY, AI resumes |

## Validation Checklist
- [ ] No audio drop during switch
- [ ] Transcript continues seamlessly
- [ ] SwitchLog has 2 entries (AI→Human, Human→AI)
- [ ] Copilot suggestions appeared for human rep
- [ ] Switch count displayed correctly in diagnostics
```

---

### Test Scenarios Matrix

| # | Scenario | Voice | Chat | Switches | Priority |
|---|----------|-------|------|----------|----------|
| 1 | AI resolves issue completely | ✅ | ✅ | 0 | P0 |
| 2 | Customer requests human once | ✅ | ✅ | 1 | P0 |
| 3 | Human hands back to AI | ✅ | ✅ | 2 | P0 |
| 4 | Multiple back-and-forth switches | ✅ | ✅ | 4+ | P1 |
| 5 | Immediate human request (no AI interaction) | ✅ | ✅ | 1 | P1 |
| 6 | Switch during AI mid-sentence | ✅ | — | 1 | P1 |
| 7 | Rapid switch spam (stress test) | ✅ | ✅ | 10+ | P2 |
| 8 | Switch with no human available | ✅ | ✅ | 1 | P2 |
| 9 | Customer hangs up during switch | ✅ | — | 1 | P2 |
| 10 | Long conversation, late switch | ✅ | ✅ | 1 | P2 |

---

### Implementation Details

**Step 9.5.1**: Add switch trigger detection to AI agent prompt (Retell)

```javascript
// In Retell agent configuration
const systemPrompt = `
...
If the user says any of these phrases, immediately trigger a handoff:
- "speak to a human"
- "talk to a person"  
- "transfer me"
- "real agent"
- "I want a human"

When handoff is triggered, say: "Of course, let me connect you with a team member right away."
Then call the transfer_to_human function.
`;
```

**Step 9.5.2**: Add chat command parser

```typescript
// src/services/chat/commandParser.ts
export function parseCommand(message: string): Command | null {
  const normalized = message.toLowerCase().trim();
  
  if (normalized === '/human' || normalized.includes('speak to agent')) {
    return { type: 'SWITCH_TO_HUMAN' };
  }
  if (normalized === '/ai' || normalized.includes('back to bot')) {
    return { type: 'SWITCH_TO_AI' };
  }
  return null;
}
```

**Step 9.5.3**: DTMF handler for voice

```typescript
// src/controllers/callController.ts
export async function handleDTMF(req: Request, res: Response) {
  const { digit, callId } = req.body;
  
  if (digit === '0') {
    await switchService.switchToHuman(callId, 'DTMF_REQUEST');
  } else if (digit === '*') {
    await switchService.switchToAI(callId, 'DTMF_REQUEST');
  }
  
  res.status(200).send();
}
```

---

### Updated Phase 9 Timeline

| Step | Task | Output |
|------|------|--------|
| 9.1 | Switch analytics query | SQL + endpoint |
| 9.2 | Analytics dashboard | Simple chart |
| 9.3 | Error handling + retries | Resilient calls |
| 9.4 | Environment validation | Startup checks |
| **9.5** | **Create test script templates** | **`/test-scripts/` folder** |
| **9.6** | **Implement DTMF handler** | **Keypress triggers** |
| **9.7** | **Implement chat command parser** | **`/human`, `/ai` commands** |
| **9.8** | **Add trigger phrases to Retell prompt** | **Voice triggers** |
| 9.9 | README with setup instructions | Documentation |
| 9.10 | Execute all P0 test scripts | QA sign-off |

---

### QA Dashboard Addition (Optional)

A simple test runner UI that:
1. Lists all test scripts
2. Lets QA mark pass/fail for each step
3. Auto-logs switch counts from the actual call
4. Compares expected vs actual switches

---

This ensures testers have clear scripts to follow and multiple ways to trigger switches during testing. Want me to proceed with **Phase 0: Foundation** now?